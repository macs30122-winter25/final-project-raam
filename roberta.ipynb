{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roberta attempt\n",
    "# Anita Sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542b922ff35c45c29663efcaadc3bb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 12:45:21.449006: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) joy 0.9382\n",
      "2) optimism 0.0362\n",
      "3) anger 0.0145\n",
      "4) sadness 0.0112\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "import torch\n",
    "import tensorflow.keras as tf_keras\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "task='emotion'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "'''\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, trust_remote_code=True)\n",
    "model.save_pretrained(MODEL)\n",
    "\n",
    "text = \"Celebrating my promotion üòé\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# # TF\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n",
    "\n",
    "text = \"Celebrating my promotion üòé\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "scores = output[0][0].numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('raam_database.db', timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('health_anxiety_table_raw',),\n",
       " ('health_anxiety_posts',),\n",
       " ('sqlite_sequence',),\n",
       " ('health_anxiety_comments',),\n",
       " ('nursing_table_raw',),\n",
       " ('nursing_posts',),\n",
       " ('nursing_comments',),\n",
       " ('teaching_table_raw',),\n",
       " ('teaching_posts',),\n",
       " ('teaching_comments',)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = '''\n",
    "    SELECT name FROM sqlite_master WHERE type=\"table\"\n",
    "    '''\n",
    "\n",
    "cur.execute(q).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'post_id', 'INTEGER', 0, None, 1),\n",
       " (1, 'timestamp', 'TEXT', 0, None, 0),\n",
       " (2, 'username', 'TEXT', 0, None, 0),\n",
       " (3, 'post_title', 'TEXT', 0, None, 0),\n",
       " (4, 'post_body', 'TEXT', 0, None, 0),\n",
       " (5, 'score', 'TEXT', 0, None, 0),\n",
       " (6, 'comments_count', 'TEXT', 0, None, 0),\n",
       " (7, 'post_url', 'TEXT', 0, None, 0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2 = '''\n",
    "    PRAGMA table_info(teaching_posts)\n",
    "    '''\n",
    "\n",
    "cur.execute(q2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = '''\n",
    "    SELECT post_body, post_id FROM teaching_posts\n",
    "    \n",
    "    '''\n",
    "\n",
    "teaching_post_lst = cur.execute(q3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'joy', 'optimism', 'sadness']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'post_id': [], 'body_text': [],\n",
    "                   'anger': [], 'joy': [],\n",
    "                   'optimism': [], 'sadness': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9187739  0.01099181 0.03515448 0.03507975] ['anger', 'joy', 'optimism', 'sadness']\n",
      "Please let the door hit you on your big dumb head on the way out!\n",
      "1) anger 0.9188\n",
      "2) optimism 0.0352\n",
      "3) sadness 0.0351\n",
      "4) joy 0.011\n",
      "[0.5995315  0.05724264 0.21298192 0.13024396] ['anger', 'joy', 'optimism', 'sadness']\n",
      "Sorry but your kids are crazy because that's how they were raised, and we get to deal with that every day, thanks!\n",
      "1) anger 0.5995\n",
      "2) optimism 0.213\n",
      "3) sadness 0.1302\n",
      "4) joy 0.0572\n",
      "[0.96682936 0.0066428  0.01227691 0.01425079] ['anger', 'joy', 'optimism', 'sadness']\n",
      "Not a drill. I have a tool kit in my classroom. I passed out hammers and wrenches and screw drivers. I was a drill sergeant. YOU flip the table up against the door. YOU here pass out scissors. YOU grab that chair. \n",
      "The kids are freshmen in high school. They were ready to fight. We were ready to fight for an hour and a half while we waited in the dark. I cussed a lot. We had a couple giggles. I was absolutely terrified I would have to make the choice between my own life and a student‚Äôs today. \n",
      "After an hour and a half, we found out it was a false threat. No gun in the building. Afterwards kids hugged me and told me what I meant to them. I tried not to cry. \n",
      "They didn‚Äôt let the kids go home after unless their parents were there. They were expected to go to class. I only had prep left. I have sat and stared and said little. I think this was a career changing moment for me. I LOVE my kids. We have a phenomenal relationship. I‚Äôm a really good teacher. But I don‚Äôt think I can ever go through this again.\n",
      "To add: it‚Äôs an inner city school where they are not new to violence. They are no stranger to the police and being in cuffs. They are a hard class to teach. I gave them the following speech: ‚ÄúListen. The police are going to come through and check us. They are going to treat you like a suspect. You put your hands up and say yes sir. I don‚Äôt give a fuck if they put you in handcuffs. They have guns. You do what your told, and I‚Äôll handle it later. If you get shot because you want to have an attitude I swear I will come to your funeral and I will make sure you are so fuckin grounded. Are you clear?‚Äù I got many ‚Äúüòßyes ma‚Äôams‚Äù in return. \n",
      "Then I said, ‚Äúnow tomorrow when I teach you about the Civil Rights movement you treat me with the same amount of respect as your treating this bad ass bitch right now. Clear?‚Äù They got a giggle out of that. \n",
      "I‚Äôm absolutely terrified to go to school tomorrow.\n",
      "1) anger 0.9668\n",
      "2) sadness 0.0143\n",
      "3) optimism 0.0123\n",
      "4) joy 0.0066\n",
      "[0.01184231 0.9114547  0.05052295 0.02618002] ['anger', 'joy', 'optimism', 'sadness']\n",
      "We are currently remote with teachers required to be in our classrooms. We did a fire drill during our online classes today. I carried my laptop outside because, duh, I had to take care of my kids. While outside, I noticed a few of my kids had carried their phones into their backyards so they could participate, too. In 21 years of teaching, it was one of my weirdest and favorite moments so far.\n",
      "1) joy 0.9115\n",
      "2) optimism 0.0505\n",
      "3) sadness 0.0262\n",
      "4) anger 0.0118\n",
      "[0.03468871 0.8969754  0.03542855 0.03290736] ['anger', 'joy', 'optimism', 'sadness']\n",
      "I teach SPED and we're in hybrid mode, so I have some groups I work with online. One of my small groups (5 kids) secretly snapped screenshots of me while I was teaching. All of the shots are weird and unflattering since I was talking, of course. Then they discreetly changed their profile pictures. \n",
      "Today they all decided to turn off their cameras at the same time so I was treated to a whole grid of different versions of my awkward talking faces, and y'all, it's the highlight of my career as an educator so far.\n",
      "1) joy 0.897\n",
      "2) optimism 0.0354\n",
      "3) anger 0.0347\n",
      "4) sadness 0.0329\n",
      "[0.07127663 0.46312386 0.41757283 0.04802673] ['anger', 'joy', 'optimism', 'sadness']\n",
      "https://www.weareteachers.com/toxic-positivity-schools/\n",
      "\"Not having a voice in reopening plans. Choosing between your children and your students. Teaching students online and in person at the same time. Working twice as hard without a pay increase. For many, this is teaching in 2020. And yes, writing ‚Äúteachers can virtually do anything‚Äù with icing and putting it on a cake in the teacher‚Äôs lounge is nice. Hearing, ‚Äúwe are all in this together,‚Äù is nice. Staff Shout-Outs on Fridays celebrating all the hard and extra work teachers are doing is nice. But you know what‚Äôs nicer? Adequate prep time during contract hours to plan. Hazard pay for teachers who are teaching in person. And how about school cultures that don‚Äôt center on toxic positivity, but teachers‚Äô physical and mental health?\"\n",
      "1) joy 0.4631\n",
      "2) optimism 0.4176\n",
      "3) anger 0.0713\n",
      "4) sadness 0.048\n",
      "[0.02397689 0.00662106 0.01346197 0.95594   ] ['anger', 'joy', 'optimism', 'sadness']\n",
      "Reddit\n",
      "I just quit mid year. Last Friday, I was offered a chance to work at a nonprofit and I took it. Same pay, but tons of flexibility, teaching adjacent, guaranteed cost of living raises, full benefits, 2 minute commute. After months paralyzing anxiety and panic attacks brought on by the worst school year ever, I am completely over the moon to be diving into a new career!\n",
      "But I can‚Äôt share my news, because every single time I tell someone they says some variation of ‚Äúthose poor kids, abandoned mid year‚Ä¶‚Äù\n",
      "And yes. I feel like shit over that. I have cried over this decision. But ultimately I decided that a sub or mid year hire is likely going to be more effective than a teacher who can barely function due to her anxiety. And at some point in my life, I have to learn to put myself first. \n",
      "So if someone tells you they are quitting mid year, please don‚Äôt make them feel even shittier about their decision. We‚Äôre all just trying to survive. \n",
      "Thanks.\n",
      "1) sadness 0.9559\n",
      "2) anger 0.024\n",
      "3) optimism 0.0135\n",
      "4) joy 0.0066\n",
      "[0.49968928 0.04050703 0.0675188  0.39228484] ['anger', 'joy', 'optimism', 'sadness']\n",
      "..... and the results were AMAZING.\n",
      "I had a student sleep until noon. Did I attempt to wake him up? Nope. Not my god damn job. Contact mom? Number hasn't worked since last year. They don't give a shit, I don't give one either.\n",
      "I have a 3rd grader unable to read/memorize letters or numbers. After doing some letter tracing worksheets, I let him have time on Youtube. Should I care about getting caught because I have multiple grade levels I have to tend to? Nope. I was put in a losing situation, so I gotta do what I gotta do. Admin doesn't like it? Maybe if my para didn't leave (inclusion) after 10:15 to END OF DAY.\n",
      "I don't plan on using curriculum for social studies or science. I'm doing Readworks and CommonLit because my students can't read for shit nor are they motivated to do the work or think. I'll read it, explain unknown words, and all I need from them is a name, date, and seven circles for their answers.\n",
      "We're in Covid Year 2. Parents didn't blink. School districts didn't blink. So, I'm going to take my work ethic down a few notches to match their level. \n",
      "Principal wants me to teach students on grade level. Oh, so that would mean 3rd ela, 4th ela, 5th ela, 3rd math, 4th math, 5th math, 3rd social, 4th social, 5th social, 3rd sci, 4th sci, and 5th sci. This is addition to the counselor having teachers teach a curriculum on trauma/social-something.\n",
      "I think I'll swipe left on that one.\n",
      "If you made it here, I thank you for your time.\n",
      "Note: still attempting to attain IEP goals.\n",
      "1) anger 0.4997\n",
      "2) sadness 0.3923\n",
      "3) optimism 0.0675\n",
      "4) joy 0.0405\n",
      "[0.06589036 0.4587683  0.4036743  0.07166696] ['anger', 'joy', 'optimism', 'sadness']\n",
      "I bought a new pack of flair pens and NO ONE CAN TAKE THEM! My students can't use them and \"forget\" to return them. I might make it a whole year without losing any!! üôå\n",
      "1) joy 0.4588\n",
      "2) optimism 0.4037\n",
      "3) sadness 0.0717\n",
      "4) anger 0.0659\n",
      "[0.21829721 0.02192708 0.1699584  0.58981735] ['anger', 'joy', 'optimism', 'sadness']\n",
      "Because they keep us too busy to apply for other jobs.\n",
      "1) sadness 0.5898\n",
      "2) anger 0.2183\n",
      "3) optimism 0.17\n",
      "4) joy 0.0219\n",
      "[0.01270431 0.01485276 0.01917123 0.9532717 ] ['anger', 'joy', 'optimism', 'sadness']\n",
      "School starts again this Monday and I don't feel refreshed or rejuvenated to go back. I'm actually dreading going back, and this is the first time that has happened! I usually am excited and ready to start again, I miss my students, etc., but this year, I am not feeling any of that. Anyone in the same boat? Any tips to help me feel more ready?\n",
      "1) sadness 0.9533\n",
      "2) optimism 0.0192\n",
      "3) joy 0.0149\n",
      "4) anger 0.0127\n",
      "[0.06582111 0.42473784 0.40764868 0.10179233] ['anger', 'joy', 'optimism', 'sadness']\n",
      "And 6 weeks left in school. \n",
      "That‚Äôs all.\n",
      "1) joy 0.4247\n",
      "2) optimism 0.4076\n",
      "3) sadness 0.1018\n",
      "4) anger 0.0658\n",
      "[0.9807339  0.00304297 0.00810476 0.00811842] ['anger', 'joy', 'optimism', 'sadness']\n",
      "When I tell an admin that my students are not signing in or doing work, I get back,‚ÄùCall home.‚Äù  That would mean that I make calls after planning that I already do not have time for or know how to do.  Why are the parents not talking to their kids?  I sent an email after email with the instuctions on how to log into the classroom and meeting.  How is it that some kids got them but not the rest? I am tired, frustrated, and pissed off.\n",
      "1) anger 0.9807\n",
      "2) sadness 0.0081\n",
      "3) optimism 0.0081\n",
      "4) joy 0.003\n",
      "[0.47727934 0.04191706 0.04882176 0.43198186] ['anger', 'joy', 'optimism', 'sadness']\n",
      "Have you guys always been this way? Unresponsive, unmotivated, disengaged? \n",
      "When I say good morning to you, it‚Äôs not code for ‚Äútell me your deepest darkest secrets and things about you that no one else knows.‚Äù It‚Äôs ‚Äúhey, good morning‚Äù and it‚Äôd be nice to get one back from you. \n",
      "When I ask if you have any questions, I don‚Äôt want you to write me a novel on your thoughts about the meaning of life. I don‚Äôt need your life story. I just want a nod or a head shake, or any indication that you‚Äôre still living and breathing because sometimes it seems like you‚Äôre not. \n",
      "I‚Äôm not asking you to build me a rocket ship or explain to me every specific detail of the beginning of the universe. I just want you to maybe acknowledge my existence for one quick second and let me know if you want to play this Kahoot I spent all night making for you. \n",
      "To the 2 or 3 people who carry their class on their backs both socially and academically, thank you for making me want to die just a little bit less each period I have you. \n",
      "To everyone else, I would also love to not do or care about anything and mindlessly stare into oblivion for 90 minutes at a time, but I can‚Äôt. I have to teach you no matter how much you don‚Äôt want to be taught, so why don‚Äôt you make this hour and a half so much easier and maybe a little more entertaining by not being a complete and utter potato?\n",
      "Thank you.\n",
      "Edit: The day after I wrote this, https://www.reddit.com/r/teaching/comments/jkkhha/small_victories_feel_so_big/?utm_source=share&utm_medium=web2x&context=3 this happened. I cried when they all left.\n",
      "1) anger 0.4773\n",
      "2) sadness 0.432\n",
      "3) optimism 0.0488\n",
      "4) joy 0.0419\n",
      "[0.06206679 0.847653   0.06580573 0.02447451] ['anger', 'joy', 'optimism', 'sadness']\n",
      "I used to teach pre-school (now a social worker), but I love you guys, and stay subscribed to this sub.\n",
      "Today is one of the first times I‚Äôve been able to be home and listen to my 6th grader‚Äôs virtual classes. His lovely science teacher asks, ‚Äúwhat claim can we make about her, with this evidence: a mug, a tea bag, hot chocolate, and coffee?‚Äù\n",
      "Of course, one little sweetie pipes up and says, ‚Äúyou drink a lot?‚Äù She is just brilliantly patient and kept her face in check, but I know deep down, that was hilarious to her. \n",
      "I‚Äôm so glad I was muted, because these teachers are trying SO FREAKIN HARD, with crappy equipment, no IT, no reliable management, kids with varying degrees of chaos and technology, I don‚Äôt have to tell YOU, you all know.... that I just burst out laughing and thought, bless her heart, I‚Äôm sure she does! üòÇ üç∑ üç∑ ü•∞\n",
      "In all seriousness, you guys are awesome; your hard work is recognized; YOU make a difference, and you‚Äôre trying your best, during scary societal collapse, with basically no resources, and I‚Äôm sure nasty/entitled/horrible parents are giving you shit, so, from one overworked/underpaid/unappreciated professional trying to better society in the face of complete chaos, to another, I love you, and I‚Äôm with you!\n",
      "In solidarity, \n",
      "A\n",
      "1) joy 0.8477\n",
      "2) optimism 0.0658\n",
      "3) anger 0.0621\n",
      "4) sadness 0.0245\n",
      "[0.8365084  0.07508738 0.0225611  0.06584323] ['anger', 'joy', 'optimism', 'sadness']\n",
      "I was ranting about how my freshmen won't follow MY directions but can execute Tiktok dances perfectly.\n",
      "1) anger 0.8365\n",
      "2) joy 0.0751\n",
      "3) sadness 0.0658\n",
      "4) optimism 0.0226\n",
      "[0.15173016 0.01726282 0.09415217 0.7368549 ] ['anger', 'joy', 'optimism', 'sadness']\n",
      "Today is the day. 2800 kids in my HS coming for face -to-face instruction. Masks optional. My classroom fits 17 social distanced and my largest class is 56. \n",
      "Nowhere to vent and I‚Äôm a bit scared and feel helpless. I don‚Äôt need to explain to this subreddit how bad it is. I‚Äôm going to do everything I can to stay safe and protect the kids. Wish me luck, all.\n",
      "Edit 1: Three periods down. Bathing in hand sanitizer. Glasses and face shield are permanently fogged.\n",
      "Edit 2: Survived the day. Bloodstream is half sanitizer. Glasses and face shield have been legally classified as fog. 3 teachers quit this morning. Not sure why they waited till the first in-person day. Perhaps to make a statement. \n",
      "Appreciate all the love, y‚Äôall.\n",
      "1) sadness 0.7369\n",
      "2) anger 0.1517\n",
      "3) optimism 0.0942\n",
      "4) joy 0.0173\n",
      "[0.02151981 0.91711694 0.05167072 0.00969259] ['anger', 'joy', 'optimism', 'sadness']\n",
      "Got back in my classroom today and checked over my roster for the upcoming school year. Ladies and gents, it's a straight-up banger. First period prep. No class sizes over 30 (even counting some students in each class opting for the online option while still on my role sheet). ALL of my favorite tenth graders requested to have me as their 11th grade history. Three TAs in the morning, all of whom are my top students from 11th grade last year. All of my 11th grade classes are advanced track, so the kids are super well behaved. AND I got approval to start a creative writing club (called The Authors Guild). \n",
      "Icing on the cake, the state wide mask mandate has seen our lowest influx of cases in a month as of today, with only 4k active cases in my city. \n",
      "I'm amped. I'm turbo charged to get back in my classroom!\n",
      "1) joy 0.9171\n",
      "2) optimism 0.0517\n",
      "3) anger 0.0215\n",
      "4) sadness 0.0097\n",
      "[0.10677914 0.20245302 0.65756744 0.03320042] ['anger', 'joy', 'optimism', 'sadness']\n",
      "I teach a class of 40-70 students. Every class I assign reading, and every class I'd come in and say, \"Does anyone have any questions about the material?\" And I'd be faced with stony silence, so I'd move on with my lecture or whatever. Recently I figured out a technique that works great for forcing student engagement in lots of circumstances, and realized it would work for this simple question.\n",
      "At the beginning of class I say, \"Take out a piece of paper. Write down one thing that you found confusing or unclear about the reading. If you totally understood it, write down one follow-up question or something you'd like to further understand. Don't hand them in. I will be calling on you randomly so write something down.\" Then after a minute or so I just say, \"Ok before I start calling on people, who has a question they want to read?\" And TA-DA!!! Tons of questions about the material.  Good ones, that had me answering questions for most of the class rather than just lecturing at the class.\n",
      "Game changer.  Maybe the folks here already do this but I think it's going to make my classes so much better.\n",
      "1) optimism 0.6576\n",
      "2) joy 0.2025\n",
      "3) anger 0.1068\n",
      "4) sadness 0.0332\n",
      "[0.31646112 0.0074161  0.01737026 0.6587525 ] ['anger', 'joy', 'optimism', 'sadness']\n",
      "I knew it would happen one day. I knew a kid would swing on me or push me. Today, 11 years of teaching in high poverty, high violence schools, it happened. \n",
      "I was pushed and fell into desks several times. I was pushed into lockers and flung to the ground. \n",
      "I was protecting my student, in my classroom from another student who shouldn't be out of class. My door doesn't latch well. Even after pushing my emergency call button twice it still took way too long for help to arrive. \n",
      "Tomorrow will be better. I will go to work, love my kids and keep changing the world. Just sad today that I couldn't keep my own students safe. \n",
      "Edit: I went to work. We didn't do anything but process the day. I've been medically checked and filed all reports. There was another fight today. After this fight was done, my students cheered for me, even though I wasn't involved. On a side note, this sub has become so toxic. At what point did it become wrong for someone to want to work?\n",
      "1) sadness 0.6588\n",
      "2) anger 0.3165\n",
      "3) optimism 0.0174\n",
      "4) joy 0.0074\n",
      "[0.04906765 0.84523404 0.06893592 0.0367624 ] ['anger', 'joy', 'optimism', 'sadness']\n",
      "I see you! Every day in the background there are brothers and sisters helping me out, asking me for online program logins or whispering the answers to their siblings <3 \n",
      "I have a brand new student with a speech IEP and his 2nd grade sister did the whole class with him, opening up his math book and showing him how to mute and unmute.\n",
      "1) joy 0.8452\n",
      "2) optimism 0.0689\n",
      "3) anger 0.0491\n",
      "4) sadness 0.0368\n",
      "[0.82642096 0.00540074 0.09384452 0.07433392] ['anger', 'joy', 'optimism', 'sadness']\n",
      "For those here saying they had wanted to be teachers and decided to go elsewhere... For those who realize how bad things are and but ‚Äúthat‚Äôs just the way it is‚Äù... I say it‚Äôs past time for accepting things because that‚Äôs the way it is or that‚Äôs how it‚Äôs always been done. You don‚Äôt like the education system the way it is? Stand with the teachers that have fought to do what they can for your children and are now being put in harms way to provide for your children knowing there is NO WAY to truly keep the students safe enough from this virus or themselves. Stand with the teachers that go to the extra length to make sure they adapt the crappy curriculum so that your child can have the best education they know how to give. Stand with the teachers that, when something terrible happens, adapts to whatever the situation is and does what is needed to provide education while doing their best to keep our kids safe. It‚Äôs time. STAND WITH THE TEACHERS!\n",
      "1) anger 0.8264\n",
      "2) optimism 0.0938\n",
      "3) sadness 0.0743\n",
      "4) joy 0.0054\n",
      "[0.01082316 0.93084174 0.0273235  0.03101158] ['anger', 'joy', 'optimism', 'sadness']\n",
      "\"How are you fairing during these unprecedented times? Good I hope? Drinking plenty of water, and resting well, yes? \" followed by a gentle hand squeeze and \"You're in my prayers.\" She sounded and looked just like mom, with the most concerned yet reassuring expression on her little 7-year-old face. Dad and I laughed, and she said \"Oh, that's just how mama greets everyone now, ya know how my mama is!\" üíÅ‚Äç‚ôÄÔ∏è I love middle school, though little instances like this remind me how much I enjoyed the little ones too!  \n",
      "I told her I was doing well and that I finished the first part of my \"big teacher test\" (I just took the 5001 Praxis last night from 9:00pm-2:00am ... gotta love those great Praxis Tests at Home time slots). She gave me a little \"celebratory dance\" and said she knew I did well because I'm \"the queen of smartie pants\" and \"smartest, bestest teacher ever\" and. y'all, I actually almost cried. Passing the elementary section was a big achievement for me, and her telling me she believed in me when no one else in my life does hit me in the feels in a way I wasn't prepared for.  üò≠üíï\n",
      "1) joy 0.9308\n",
      "2) sadness 0.031\n",
      "3) optimism 0.0273\n",
      "4) anger 0.0108\n",
      "[0.96673465 0.00331641 0.01227225 0.01767672] ['anger', 'joy', 'optimism', 'sadness']\n",
      "i clean 20-30 classrooms, and i‚Äôm only given four hours to do so. if a room isn‚Äôt clean, maybe consider the state you left it in. wipe down a whiteboard or two, check the coffee machine and see if there‚Äôs anything you can throw in the bin, encourage kids to leave with you. we work hard so you guys have a space to teach, and 90% of the time you‚Äôll only know who we are if you want to complain.\n",
      "i get being a teacher is hard, and the pay is shit and the kids are assholes, but maybe spare a thought once or twice for those on less than minimum wage having to scrape banana gunk from your desk because ‚Äúit‚Äôs fine, the cleaner will get it‚Äù\n",
      "1) anger 0.9667\n",
      "2) sadness 0.0177\n",
      "3) optimism 0.0123\n",
      "4) joy 0.0033\n",
      "[0.0365255  0.78106385 0.16485327 0.01755736] ['anger', 'joy', 'optimism', 'sadness']\n",
      "I used to spend multiple class periods trying to teach latitude and longitude only to give up after a week with half the class still not getting it. Today, I was able to get 80% of my students labeling which city lies at which location with 100% accuracy. I am PROUD! My kids this year are awesome!\n",
      "1) joy 0.7811\n",
      "2) optimism 0.1649\n",
      "3) anger 0.0365\n",
      "4) sadness 0.0176\n",
      "[0.17840126 0.46068865 0.21674323 0.14416684] ['anger', 'joy', 'optimism', 'sadness']\n",
      "[www.mrsbrown.art/drive](www.mrsbrown.art/drive)\n",
      "1) joy 0.4607\n",
      "2) optimism 0.2167\n",
      "3) anger 0.1784\n",
      "4) sadness 0.1442\n",
      "[0.8982311  0.00755647 0.01350827 0.08070422] ['anger', 'joy', 'optimism', 'sadness']\n",
      "Pardon the rant. I'm over it.\n",
      "I am irritated that parents aren‚Äôt letting their kids fail. I don‚Äôt have kids, but I was a kid whose mom let me fail, and miraculously, I learned from it and never made that mistake again. It was in 7th grade, the grade I currently teach. I waited until literally the night before our science fair project was due to begin it. My mom did not help. My mom did not write email after email to my teacher making excuses, begging for more time, and spinning a sob story of how hard I tried. No, my mom let me fail. I didn‚Äôt ‚Äúfail.‚Äù I probably earned a C, but for a student who typically earned As, it was a failure to me. While I may have procrastinated a few times after, it was never to the extent of what happened with that science fair project. Parents: You are failing your child by not letting them fail. One ‚ÄúC‚Äù in middle school isn‚Äôt the end of the world, and it‚Äôs much better to earn that C in middle school rather than in high school where the GPA counts more heavily. GPA isn‚Äôt really a thing in middle school until 8th grade. Your child should not have waited until the DAY BEFORE THE EXTRA CREDIT was due to begin it. Your child should know how to check and read his emails. Your child should know that he will not get credit for plagiarizing his assignment. Your child would have those horrible feelings of failure and disappointment to motivate him not to make those mistakes again if you hadn‚Äôt sent me a barrage of emails, if your husband, a 35-year veteran teacher who apparently has no clue what plagiarism is, hadn‚Äôt sent me emails, and if administration hadn‚Äôt stepped in to force my hand to offer your precious baby a second chance on extra credit. In the future, just tell me what grade you want your kid to have. It will save both of us time and frustration.\n",
      "1) anger 0.8982\n",
      "2) sadness 0.0807\n",
      "3) optimism 0.0135\n",
      "4) joy 0.0076\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'embeddings' (type TFRobertaEmbeddings).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,512] = 514 is not in [0, 514) [Op:ResourceGather] name: \n\nCall arguments received by layer 'embeddings' (type TFRobertaEmbeddings):\n  ‚Ä¢ input_ids=tf.Tensor(shape=(1, 682), dtype=int32)\n  ‚Ä¢ position_ids=None\n  ‚Ä¢ token_type_ids=tf.Tensor(shape=(1, 682), dtype=int32)\n  ‚Ä¢ inputs_embeds=None\n  ‚Ä¢ past_key_values_length=0\n  ‚Ä¢ training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m weird_tup[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m tokenizer(string, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m scores \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      7\u001b[0m scores \u001b[38;5;241m=\u001b[39m softmax(scores)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_tf_roberta.py:1432\u001b[0m, in \u001b[0;36mTFRobertaForSequenceClassification.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(ROBERTA_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1425\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFSequenceClassifierOutput, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;124;03m    labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;124;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;124;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;124;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1432\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1440\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1444\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1445\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output, training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_tf_roberta.py:745\u001b[0m, in \u001b[0;36mTFRobertaMainLayer.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    743\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfill(dims\u001b[38;5;241m=\u001b[39minput_shape, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 745\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;66;03m# We create a 3D attention mask from a 2D tensor mask.\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;66;03m# Sizes are [batch_size, 1, 1, to_seq_length]\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;66;03m# this attention mask is more simple than the triangular masking of causal attention\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;66;03m# used in OpenAI GPT, we just need to prepare the broadcast dimension here.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m attention_mask_shape \u001b[38;5;241m=\u001b[39m shape_list(attention_mask)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_tf_roberta.py:164\u001b[0m, in \u001b[0;36mTFRobertaEmbeddings.call\u001b[0;34m(self, input_ids, position_ids, token_type_ids, inputs_embeds, past_key_values_length, training)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m         position_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(\n\u001b[1;32m    161\u001b[0m             tf\u001b[38;5;241m.\u001b[39mrange(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, limit\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    162\u001b[0m         )\n\u001b[0;32m--> 164\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m token_type_embeds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings, indices\u001b[38;5;241m=\u001b[39mtoken_type_ids)\n\u001b[1;32m    166\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds \u001b[38;5;241m+\u001b[39m token_type_embeds\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'embeddings' (type TFRobertaEmbeddings).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,512] = 514 is not in [0, 514) [Op:ResourceGather] name: \n\nCall arguments received by layer 'embeddings' (type TFRobertaEmbeddings):\n  ‚Ä¢ input_ids=tf.Tensor(shape=(1, 682), dtype=int32)\n  ‚Ä¢ position_ids=None\n  ‚Ä¢ token_type_ids=tf.Tensor(shape=(1, 682), dtype=int32)\n  ‚Ä¢ inputs_embeds=None\n  ‚Ä¢ past_key_values_length=0\n  ‚Ä¢ training=False"
     ]
    }
   ],
   "source": [
    "for weird_tup in teaching_post_lst:\n",
    "    string = weird_tup[0]\n",
    "    id = weird_tup[1]\n",
    "    tokenized = tokenizer(string, return_tensors='tf')\n",
    "    output = model(tokenized)\n",
    "    scores = output[0][0].numpy()\n",
    "    scores = softmax(scores)\n",
    "    print(scores, labels)\n",
    "    df.loc[len(df)] = [id, string, np.nan, np.nan, np.nan, np.nan]\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    print(string)\n",
    "\n",
    "    df.loc[len(df)-1, labels] = scores\n",
    "\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = labels[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "        #df.iloc[-1, l] = s\n",
    "        print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>body_text</th>\n",
       "      <th>anger</th>\n",
       "      <th>joy</th>\n",
       "      <th>optimism</th>\n",
       "      <th>sadness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Please let the door hit you on your big dumb h...</td>\n",
       "      <td>0.918774</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>0.035154</td>\n",
       "      <td>0.035080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sorry but your kids are crazy because that's h...</td>\n",
       "      <td>0.599531</td>\n",
       "      <td>0.057243</td>\n",
       "      <td>0.212982</td>\n",
       "      <td>0.130244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Not a drill. I have a tool kit in my classroom...</td>\n",
       "      <td>0.966829</td>\n",
       "      <td>0.006643</td>\n",
       "      <td>0.012277</td>\n",
       "      <td>0.014251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>We are currently remote with teachers required...</td>\n",
       "      <td>0.011842</td>\n",
       "      <td>0.911455</td>\n",
       "      <td>0.050523</td>\n",
       "      <td>0.026180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I teach SPED and we're in hybrid mode, so I ha...</td>\n",
       "      <td>0.034689</td>\n",
       "      <td>0.896975</td>\n",
       "      <td>0.035429</td>\n",
       "      <td>0.032907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                          body_text     anger  \\\n",
       "0        1  Please let the door hit you on your big dumb h...  0.918774   \n",
       "1        2  Sorry but your kids are crazy because that's h...  0.599531   \n",
       "2        3  Not a drill. I have a tool kit in my classroom...  0.966829   \n",
       "3        4  We are currently remote with teachers required...  0.011842   \n",
       "4        5  I teach SPED and we're in hybrid mode, so I ha...  0.034689   \n",
       "\n",
       "        joy  optimism   sadness  \n",
       "0  0.010992  0.035154  0.035080  \n",
       "1  0.057243  0.212982  0.130244  \n",
       "2  0.006643  0.012277  0.014251  \n",
       "3  0.911455  0.050523  0.026180  \n",
       "4  0.896975  0.035429  0.032907  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
